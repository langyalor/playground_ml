{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################################\n",
    "#       d888888b                         d888888b\n",
    "#    d888    8888b                    d888888   888b\n",
    "#  d88    88  898888b               d8888  888     88b\n",
    "# d8P        88888888b             d88888888888     b8b\n",
    "# 88        8888888888             88888888888       88\n",
    "# 88       88888888888             8888888888        88\n",
    "# 98b     88888888888P             988888888        d8P\n",
    "#  988     888  8888P      _=_      9888898  88    88P\n",
    "#    9888   888888P      q(-_-)p       98888    888P\n",
    "#       9888888P         '_) (_`         9888888P\n",
    "#          88            /__/  \\            88\n",
    "#          88          _(<_   / )_          88\n",
    "#         d88b        (__\\_\\_|_/__)        d88b\n",
    "###\n",
    "###                  佛祖保佑，Bug退去；\n",
    "###                  阿弥陀佛，善哉善哉\n",
    "# ###############################################################\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4e3d1982927f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/opt/ros/kinetic/lib/python2.7/dist-packages'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 图像预处理\n",
    "def preprocess(file_name):\n",
    "    img = cv2.imread(file_name)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    chs = cv2.split(img)\n",
    "    hue = ((chs[0] > 30) * 1) * chs[0]\n",
    "    res = ((hue < 50) * 1) * hue\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    res = cv2.morphologyEx(res.astype(np.float32), cv2.MORPH_ERODE, kernel)\n",
    "    resize = cv2.resize(res, (196, 196))\n",
    "    resize = resize.reshape((1, 196, 196))\n",
    "    resize = cv2.normalize(resize, resize, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "#     print(file_name)\n",
    "#     cv2.imwrite(\"test.jpg\", resize[0])\n",
    "\n",
    "    return resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/media/lor/data/data/plant-seedlings-classification/train'\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "label_to_idx = {}\n",
    "for (i, cat) in enumerate(os.listdir(train_path)):\n",
    "    label_to_idx[cat] = i\n",
    "    cat_path = os.path.join(train_path, cat)\n",
    "#     print(cat_path)\n",
    "    for item in os.listdir(cat_path):\n",
    "        train_labels.append(i)\n",
    "        train_samples.append(os.path.join(cat_path, item))\n",
    "# print(len(train_labels))\n",
    "# print(len(train_samples))\n",
    "train_labels.extend(train_labels)\n",
    "train_samples.extend(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, targets):\n",
    "        self.samples = samples\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target = self.targets[idx]\n",
    "        sample = preprocess(self.samples[idx])\n",
    "        if idx >= 4750:\n",
    "            sample = cv2.flip(sample, 0)\n",
    "        sample = sample / 255\n",
    "        return (target, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PlantDS(train_samples, train_labels)\n",
    "indices = list(range(train_ds.__len__()))\n",
    "np.random.shuffle(indices)\n",
    "train_indices = indices[0:9000]\n",
    "valid_indices = indices[9000:9500]\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dloader = torch.utils.data.DataLoader(train_ds, batch_size=8, num_workers=4, sampler=train_sampler)\n",
    "valid_dloader = torch.utils.data.DataLoader(train_ds, batch_size=1, num_workers=4, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PlantNet, self).__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "                        nn.Conv2d(1, 8, kernel_size=(3, 3), stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(8),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.Conv2d(8, 12, kernel_size=(5, 5), stride=1, padding=2),\n",
    "                        nn.BatchNorm2d(12),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "                        nn.Conv2d(12, 18, kernel_size=(5, 5), stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(18),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "                        nn.Conv2d(18, 24, kernel_size=(5, 5), stride=1, padding=2),\n",
    "                        nn.BatchNorm2d(24),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=(3, 3), stride=3),\n",
    "        )\n",
    "        \n",
    "        self.layer_exp_1 = nn.Sequential(\n",
    "                        nn.Conv2d(24, 48, kernel_size=(3, 3), stride=1, padding=1),\n",
    "                        nn.BatchNorm2d(48),\n",
    "                        nn.LeakyReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "        )\n",
    "        \n",
    "        self.layer_3 = nn.Linear(8 * 8 * 48, 1024)\n",
    "        self.layer_4 = nn.Linear(1024, 341)\n",
    "#         self.layer_5 = nn.Linear(341, 113)\n",
    "        self.layer_6 = nn.Linear(341, 12)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.layer_exp_1(out)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.layer_4(F.relu(out))\n",
    "#         out = self.layer_5(F.relu(out))\n",
    "        out = self.layer_6(F.relu(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "net = PlantNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "total_step = len(train_dloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1125], Loss: 1.1408\n",
      "Epoch [1/10], Step [200/1125], Loss: 1.1259\n",
      "Epoch [1/10], Step [300/1125], Loss: 1.9303\n",
      "Epoch [1/10], Step [400/1125], Loss: 2.2203\n",
      "Epoch [1/10], Step [500/1125], Loss: 1.7562\n",
      "Epoch [1/10], Step [600/1125], Loss: 1.4677\n",
      "Epoch [1/10], Step [700/1125], Loss: 1.2713\n",
      "Epoch [1/10], Step [800/1125], Loss: 1.7741\n",
      "Epoch [1/10], Step [900/1125], Loss: 0.8069\n",
      "Epoch [1/10], Step [1000/1125], Loss: 1.1476\n",
      "Epoch [1/10], Step [1100/1125], Loss: 1.4585\n",
      "Epoch [2/10], Step [100/1125], Loss: 1.0584\n",
      "Epoch [2/10], Step [200/1125], Loss: 0.5797\n",
      "Epoch [2/10], Step [300/1125], Loss: 0.6667\n",
      "Epoch [2/10], Step [400/1125], Loss: 1.1067\n",
      "Epoch [2/10], Step [500/1125], Loss: 0.9751\n",
      "Epoch [2/10], Step [600/1125], Loss: 0.8032\n",
      "Epoch [2/10], Step [700/1125], Loss: 0.6883\n",
      "Epoch [2/10], Step [800/1125], Loss: 1.7047\n",
      "Epoch [2/10], Step [900/1125], Loss: 0.6385\n",
      "Epoch [2/10], Step [1000/1125], Loss: 0.7781\n",
      "Epoch [2/10], Step [1100/1125], Loss: 0.3574\n",
      "Epoch [3/10], Step [100/1125], Loss: 0.2848\n",
      "Epoch [3/10], Step [200/1125], Loss: 0.7887\n",
      "Epoch [3/10], Step [300/1125], Loss: 0.9696\n",
      "Epoch [3/10], Step [400/1125], Loss: 0.5453\n",
      "Epoch [3/10], Step [500/1125], Loss: 1.3704\n",
      "Epoch [3/10], Step [600/1125], Loss: 0.4462\n",
      "Epoch [3/10], Step [700/1125], Loss: 0.0402\n",
      "Epoch [3/10], Step [800/1125], Loss: 0.8809\n",
      "Epoch [3/10], Step [900/1125], Loss: 1.2451\n",
      "Epoch [3/10], Step [1000/1125], Loss: 0.1786\n",
      "Epoch [3/10], Step [1100/1125], Loss: 0.2390\n",
      "Epoch [4/10], Step [100/1125], Loss: 0.3913\n",
      "Epoch [4/10], Step [200/1125], Loss: 0.3823\n",
      "Epoch [4/10], Step [300/1125], Loss: 1.5434\n",
      "Epoch [4/10], Step [400/1125], Loss: 0.5782\n",
      "Epoch [4/10], Step [500/1125], Loss: 0.7760\n",
      "Epoch [4/10], Step [600/1125], Loss: 0.6341\n",
      "Epoch [4/10], Step [700/1125], Loss: 0.3635\n",
      "Epoch [4/10], Step [800/1125], Loss: 0.5217\n",
      "Epoch [4/10], Step [900/1125], Loss: 0.2105\n",
      "Epoch [4/10], Step [1000/1125], Loss: 0.2081\n",
      "Epoch [4/10], Step [1100/1125], Loss: 0.1172\n",
      "Epoch [5/10], Step [100/1125], Loss: 0.1693\n",
      "Epoch [5/10], Step [200/1125], Loss: 0.1985\n",
      "Epoch [5/10], Step [300/1125], Loss: 0.5829\n",
      "Epoch [5/10], Step [400/1125], Loss: 0.1318\n",
      "Epoch [5/10], Step [500/1125], Loss: 0.4085\n",
      "Epoch [5/10], Step [600/1125], Loss: 1.2220\n",
      "Epoch [5/10], Step [700/1125], Loss: 0.6776\n",
      "Epoch [5/10], Step [800/1125], Loss: 0.0572\n",
      "Epoch [5/10], Step [900/1125], Loss: 0.6859\n",
      "Epoch [5/10], Step [1000/1125], Loss: 0.8239\n",
      "Epoch [5/10], Step [1100/1125], Loss: 0.0766\n",
      "Epoch [6/10], Step [100/1125], Loss: 0.0790\n",
      "Epoch [6/10], Step [200/1125], Loss: 0.5261\n",
      "Epoch [6/10], Step [300/1125], Loss: 0.4439\n",
      "Epoch [6/10], Step [400/1125], Loss: 0.3360\n",
      "Epoch [6/10], Step [500/1125], Loss: 0.2517\n",
      "Epoch [6/10], Step [600/1125], Loss: 0.1181\n",
      "Epoch [6/10], Step [700/1125], Loss: 0.1721\n",
      "Epoch [6/10], Step [800/1125], Loss: 0.2011\n",
      "Epoch [6/10], Step [900/1125], Loss: 0.0642\n",
      "Epoch [6/10], Step [1000/1125], Loss: 0.6801\n",
      "Epoch [6/10], Step [1100/1125], Loss: 0.4137\n",
      "Epoch [7/10], Step [100/1125], Loss: 0.3938\n",
      "Epoch [7/10], Step [200/1125], Loss: 0.2289\n",
      "Epoch [7/10], Step [300/1125], Loss: 0.4248\n",
      "Epoch [7/10], Step [400/1125], Loss: 0.4059\n",
      "Epoch [7/10], Step [500/1125], Loss: 0.9278\n",
      "Epoch [7/10], Step [600/1125], Loss: 0.2042\n",
      "Epoch [7/10], Step [700/1125], Loss: 0.1842\n",
      "Epoch [7/10], Step [800/1125], Loss: 0.1320\n",
      "Epoch [7/10], Step [900/1125], Loss: 0.1364\n",
      "Epoch [7/10], Step [1000/1125], Loss: 0.0465\n",
      "Epoch [7/10], Step [1100/1125], Loss: 0.0468\n",
      "Epoch [8/10], Step [100/1125], Loss: 0.0087\n",
      "Epoch [8/10], Step [200/1125], Loss: 0.9196\n",
      "Epoch [8/10], Step [300/1125], Loss: 0.4035\n",
      "Epoch [8/10], Step [400/1125], Loss: 0.0785\n",
      "Epoch [8/10], Step [500/1125], Loss: 0.1127\n",
      "Epoch [8/10], Step [600/1125], Loss: 0.3746\n",
      "Epoch [8/10], Step [700/1125], Loss: 0.1147\n",
      "Epoch [8/10], Step [800/1125], Loss: 0.0054\n",
      "Epoch [8/10], Step [900/1125], Loss: 0.0140\n",
      "Epoch [8/10], Step [1000/1125], Loss: 0.2490\n",
      "Epoch [8/10], Step [1100/1125], Loss: 0.2437\n",
      "Epoch [9/10], Step [100/1125], Loss: 0.0083\n",
      "Epoch [9/10], Step [200/1125], Loss: 0.0353\n",
      "Epoch [9/10], Step [300/1125], Loss: 0.0123\n",
      "Epoch [9/10], Step [400/1125], Loss: 0.0066\n",
      "Epoch [9/10], Step [500/1125], Loss: 0.0201\n",
      "Epoch [9/10], Step [600/1125], Loss: 0.0120\n",
      "Epoch [9/10], Step [700/1125], Loss: 0.0520\n",
      "Epoch [9/10], Step [800/1125], Loss: 0.0312\n",
      "Epoch [9/10], Step [900/1125], Loss: 0.0017\n",
      "Epoch [9/10], Step [1000/1125], Loss: 0.0021\n",
      "Epoch [9/10], Step [1100/1125], Loss: 0.0471\n",
      "Epoch [10/10], Step [100/1125], Loss: 0.0285\n",
      "Epoch [10/10], Step [200/1125], Loss: 0.0054\n",
      "Epoch [10/10], Step [300/1125], Loss: 0.0070\n",
      "Epoch [10/10], Step [400/1125], Loss: 0.0532\n",
      "Epoch [10/10], Step [500/1125], Loss: 0.0006\n",
      "Epoch [10/10], Step [600/1125], Loss: 0.0028\n",
      "Epoch [10/10], Step [700/1125], Loss: 0.0004\n",
      "Epoch [10/10], Step [800/1125], Loss: 0.6597\n",
      "Epoch [10/10], Step [900/1125], Loss: 0.6375\n",
      "Epoch [10/10], Step [1000/1125], Loss: 0.2834\n",
      "Epoch [10/10], Step [1100/1125], Loss: 0.0042\n",
      "Train spending: 38:10.81933856010437\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (target, sample) in enumerate(train_dloader):\n",
    "        pred = net(sample)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "end = time.time()\n",
    "interval = end - start\n",
    "print(\"Train spending: {}:{}\".format(math.floor(interval / 60), interval - math.floor(interval / 60) * 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.0%\n",
      "Test spending: 6.253731727600098\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "net.eval()\n",
    "total = 0\n",
    "for (target, sample) in valid_dloader:\n",
    "    pred = net(sample)\n",
    "    total = total + (torch.argmax(pred, 1) == target).sum().item()\n",
    "\n",
    "end = time.time()\n",
    "interval = end - start\n",
    "print(\"Accuracy: {0}%\".format(100 * total / len(valid_indices)))\n",
    "print(\"Test spending: {}\".format(interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
